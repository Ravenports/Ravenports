# Buildsheet autogenerated by ravenadm tool -- Do not edit.

NAMEBASE=		python-charset-normalizer
VERSION=		2.1.1
KEYWORDS=		python
VARIANTS=		py39 py310
SDESC[py310]=		Charset Detection, for Everyone (3.10)
SDESC[py39]=		Charset Detection, for Everyone (3.9)
HOMEPAGE=		https://github.com/ousret/charset_normalizer
CONTACT=		Python_Automaton[python@ironwolf.systems]

DOWNLOAD_GROUPS=	main
SITES[main]=		PYPIWHL/db/51/a507c856293ab05cdc1db77ff4bc1268ddd39f29e7dc4919aa497f0adbec
DISTFILE[1]=		charset_normalizer-2.1.1-py3-none-any.whl:main
DF_INDEX=		1
SPKGS[py310]=		single
SPKGS[py39]=		single

OPTIONS_AVAILABLE=	PY39 PY310
OPTIONS_STANDARD=	none
VOPTS[py310]=		PY39=OFF PY310=ON
VOPTS[py39]=		PY39=ON PY310=OFF

DISTNAME=		charset_normalizer-2.1.1.dist-info

GENERATED=		yes

[PY39].USES_ON=				python:py39,wheel

[PY310].USES_ON=			python:py310,wheel

[FILE:3387:descriptions/desc.single]

<h1 align="center">Charset Detection, for Everyone üëã [image]</h1>

<p align="center">
  <sup>The Real First Universal Charset Detector</sup><br>
  
    [image]
  
  
      [image]
  
  
    [image]
  
</p>

> A library that helps you read text from an unknown charset encoding.<br
/> Motivated by `chardet`,
> I'm trying to resolve the issue by taking a new approach.
> All IANA character set names for which the Python core library provides
codecs are supported.

<p align="center">
  >>>>> üëâ Try Me Online Now, Then Adopt Me üëà  <<<<<
</p>

This project offers you an alternative to **Universal Charset Encoding
Detector**, also known as **Chardet**.

| Feature       | [Chardet]       | Charset Normalizer | [cChardet] |
| ------------- | :-------------: | :------------------: |
:------------------: |
| `Fast`         | ‚ùå<br>          | ‚úÖ<br>             | ‚úÖ <br> |
| `Universal**`     | ‚ùå            | ‚úÖ                 | ‚ùå |
| `Reliable` **without** distinguishable standards | ‚ùå | ‚úÖ | ‚úÖ |
| `Reliable` **with** distinguishable standards | ‚úÖ | ‚úÖ | ‚úÖ |
| `License` | LGPL-2.1<br>_restrictive_ | MIT | MPL-1.1<br>_restrictive_ |
| `Native Python` | ‚úÖ | ‚úÖ | ‚ùå |
| `Detect spoken language` | ‚ùå | ‚úÖ | N/A |
| `UnicodeDecodeError Safety` | ‚ùå | ‚úÖ | ‚ùå |
| `Whl Size` | 193.6 kB | 39.5 kB | ~200 kB |
| `Supported Encoding` | 33 | :tada: [93]  | 40

<p align="center">
[image][image]

*\*\* : They are clearly using specific code for a specific encoding even
if covering most of used one*<br> 
Did you got there because of the logs? See
[https://charset-normalizer.readthedocs.io/en/latest/user/miscellaneous.html]

## ‚≠ê Your support

*Fork, test-it, star-it, submit your ideas! We do listen.*
  
## ‚ö° Performance

This package offer better performance than its counterpart Chardet. Here
are some numbers.

| Package       | Accuracy       | Mean per file (ms) | File per sec (est)
|
| ------------- | :-------------: | :------------------: |
:------------------: |
|      [chardet]        |     86 %     |     200 ms      |       5 file/sec
       |
| charset-normalizer |    **98 %**     |     **39 ms**      |       26
file/sec    |

| Package       | 99th percentile       | 95th percentile | 50th percentile
|
| ------------- | :-------------: | :------------------: |
:------------------: |
|      [chardet]        |     1200 ms     |     287 ms      |       23 ms  
     |
| charset-normalizer |    400 ms     |     200 ms      |       15 ms    |

Chardet's performance on larger file (1MB+) are very poor. Expect huge
difference on large payload.

> Stats are generated using 400+ files using default parameters. More
details on used files, see GHA workflows.
> And yes, these results might change at any time. The dataset can be
updated to include more files.
> The actual delays heavily depends on your CPU capabilities. The factors
should remain the same.
> Keep in mind that the stats are generous and that Chardet accuracy vs our
is measured using Chardet initial capability
> (eg. Supported Encoding) Challenge-them if you want.

[cchardet] is a non-native (cpp binding) and unmaintained faster
alternative with 
a better accuracy than chardet but lower than this package. If speed is the
most important factor, you should try it.

## ‚ú® Installation

Using PyPi for latest stable
```sh
pip install charset-normalizer -U


[FILE:120:distinfo]
83e9a75d1911279afd89352c68b45348559d1fc0506b054b346651b5e7fee29f        39748 charset_normalizer-2.1.1-py3-none-any.whl

